{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Forecasting using weather station data\n",
    "\n",
    "In this notebook, we will build a forecasting model for the weather station data selected.\n",
    "\n",
    "**Goal**: Predict Temperature for the **next 12 hours** using **24 hours** of history."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will explore three model architectures:\n",
    "\n",
    "1.  **Linear Regression (The Baseline)**:\n",
    "    *   *Theory*: \"Tomorrow is just a weighted average of the past.\"\n",
    "    *   *Pros*: Fast, Interpretable.\n",
    "    *   *Cons*: Limited power; cannot learn complex interactions (e.g., \"If wind is high AND sun is out, temp rises fast\").\n",
    "\n",
    "2.  **MLP (Multi-Layer Perceptron)**:\n",
    "    *   *Theory*: \"Non-linear pattern matching.\" Adds hidden layers and activation functions (ReLU) to bend the lines.\n",
    "    *   *Pros*: Can model complex curves.\n",
    "    *   *Cons*: Treats the history as a flat vector. It doesn't inherently understand that Hour 1 came before Hour 2.\n",
    "\n",
    "3.  **LSTM (Long Short-Term Memory)**:\n",
    "    *   *Theory*: \"Sequence Modeling.\" Processes data step-by-step, maintaining an internal \"state\" or memory.\n",
    "    *   *Pros*: Designed for time series; understands causality and long-term dependencies.\n",
    "    *   *Cons*: Slower to train.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Data Preparation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "except:\n",
    "    IN_COLAB = False\n",
    "\n",
    "if IN_COLAB:\n",
    "    print(\"Running on Google Colab. Installing dependencies...\")\n",
    "    !pip install lightning\n",
    "else:\n",
    "    print(\"Running locally. Skipping dependency installation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data\n",
    "We load the processed hourly weather data for SFO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '../data/processed/sfo_hourly_2022_2026.csv'\n",
    "data = pd.read_csv(data_path, index_col='time', parse_dates=True)\n",
    "\n",
    "print(f\"Loaded {len(data)} rows.\")\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before feeding data into models, we must decide what the model should \"see\".\n",
    "\n",
    "*   **Temperature (temp)**: Driven by solar radiation and air mass movements.\n",
    "*   **Humidity (rhum)**: Critical for cloud formation and thermal comfort.\n",
    "*   **Pressure (pres)**: Large-scale high/low pressure systems drive wind and weather patterns.\n",
    "*   **Wind Speed (wspd)**: Advection (moving heat around).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_FEATURES = [\n",
    "    'pres', # Pressure\n",
    "    'rhum', # Humidity\n",
    "    'wspd', # Wind Speed\n",
    "    'temp', # Temperature\n",
    "]\n",
    "\n",
    "TARGET_FEATURE = 'temp' # We want to forecast Temperature\n",
    "\n",
    "# Configuration for Time Sequence\n",
    "HISTORY_HOURS = 24\n",
    "FORECAST_HOURS = 12\n",
    "\n",
    "input_dim = len(INPUT_FEATURES)\n",
    "print(f\"Inputs ({input_dim} features): {INPUT_FEATURES}\")\n",
    "print(f\"Target: {TARGET_FEATURE}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time-Based Splitting & Scaling\n",
    "\n",
    "We cannot shuffle time series data randomly. We fit the Scaler ONLY on the training set to simulate a real-world scenario (we don't know the future range of values)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_date = '2025-01-01'\n",
    "\n",
    "# 1. Split Train+Val (Past) vs Test (Future)\n",
    "train_val_df = data[data.index < split_date]\n",
    "test_df = data[data.index >= split_date]\n",
    "\n",
    "# 2. Further Split Train vs Validation (80/20 chronological)\n",
    "n_train_val = len(train_val_df)\n",
    "train_end = int(n_train_val * 0.8)\n",
    "\n",
    "train_df = train_val_df.iloc[:train_end]\n",
    "val_df = train_val_df.iloc[train_end:]\n",
    "\n",
    "print(f\"Train: {len(train_df)} rows\")\n",
    "print(f\"Val:   {len(val_df)} rows\")\n",
    "print(f\"Test:  {len(test_df)} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Feature Scaling (Independent for X and y)\n",
    "X_scaler = StandardScaler()\n",
    "y_scaler = StandardScaler()\n",
    "\n",
    "# Fit ONLY on Training Data\n",
    "X_train = X_scaler.fit_transform(train_df[INPUT_FEATURES]).astype(np.float32)\n",
    "y_train = y_scaler.fit_transform(train_df[[TARGET_FEATURE]]).astype(np.float32)\n",
    "\n",
    "# Transform Validation and Test using Training statistics\n",
    "X_val = X_scaler.transform(val_df[INPUT_FEATURES]).astype(np.float32)\n",
    "y_val = y_scaler.transform(val_df[[TARGET_FEATURE]]).astype(np.float32)\n",
    "\n",
    "X_test = X_scaler.transform(test_df[INPUT_FEATURES]).astype(np.float32)\n",
    "y_test = y_scaler.transform(test_df[[TARGET_FEATURE]]).astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to chop our continuous timeline into samples:\n",
    "*   **Input**: Window of length `HISTORY_HOURS`.\n",
    "*   **Target**: The *following* window of length `FORECAST_HOURS`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeatherDataset(Dataset):\n",
    "    def __init__(self, X, y, input_len, output_len):\n",
    "        self.X = torch.tensor(X)\n",
    "        self.y = torch.tensor(y)\n",
    "        self.input_len = input_len\n",
    "        self.output_len = output_len\n",
    "\n",
    "    def __len__(self):\n",
    "        # We stop when we can't form a full future window\n",
    "        return len(self.X) - self.input_len - self.output_len\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Input Sequence\n",
    "        x_seq = self.X[idx : idx + self.input_len]\n",
    "        \n",
    "        # Target Sequence (immediately following input)\n",
    "        y_seq = self.y[idx + self.input_len : idx + self.input_len + self.output_len]\n",
    "        \n",
    "        # Remove channel dim for target: (24, 1) -> (24)\n",
    "        return x_seq, y_seq.squeeze(-1)\n",
    "\n",
    "INPUT_LEN = HISTORY_HOURS\n",
    "OUTPUT_LEN = FORECAST_HOURS\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "train_ds = WeatherDataset(X_train, y_train, INPUT_LEN, OUTPUT_LEN)\n",
    "val_ds = WeatherDataset(X_val, y_val, INPUT_LEN, OUTPUT_LEN)\n",
    "test_ds = WeatherDataset(X_test, y_test, INPUT_LEN, OUTPUT_LEN)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True) # Shuffle Train!\n",
    "val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. The Training Loop (PyTorch Lightning)\n",
    "\n",
    "Instead of writing manual training loops, we use `LightningModule`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeatherPredictor(pl.LightningModule):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.criterion = nn.MSELoss()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        loss = self.criterion(self(x), y)\n",
    "        self.log(\"train_loss\", loss, prog_bar=True)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        loss = self.criterion(self(x), y)\n",
    "        self.log(\"val_loss\", loss, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return optim.Adam(self.parameters(), lr=0.001)\n",
    "\n",
    "# Convenience function to train & save best model\n",
    "def train_model(model, name, train_loader, val_loader):\n",
    "    print(f\"\\n=== Training {name} ===\")\n",
    "    predictor = WeatherPredictor(model)\n",
    "    \n",
    "    # Stop if validation loss doesn't improve for 5 epochs\n",
    "    early_stop = EarlyStopping(monitor=\"val_loss\", patience=5)\n",
    "    checkpoint = ModelCheckpoint(monitor=\"val_loss\", dirpath=\"checkpoints\", filename=f\"{name}-best\")\n",
    "    \n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=50,\n",
    "        callbacks=[early_stop, checkpoint], \n",
    "        accelerator='auto',\n",
    "        devices=1, \n",
    "        logger=False,\n",
    "        enable_progress_bar=True\n",
    "    )\n",
    "    trainer.fit(predictor, train_loader, val_loader)\n",
    "    \n",
    "    # Load best weights\n",
    "    best = WeatherPredictor.load_from_checkpoint(checkpoint.best_model_path, model=model)\n",
    "    return best.model\n",
    "\n",
    "trained_models = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model 1: Linear Regression\n",
    "\n",
    "![lr](imgs/linear_regression.png)\n",
    "\n",
    "The simplest approach. We flatten the history into a single vector and learn a weight for every input point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearModel(nn.Module):\n",
    "    def __init__(self, input_dim, input_len, output_len):\n",
    "        super().__init__()\n",
    "        self.flatten_dim = input_dim * input_len\n",
    "        self.fc = nn.Linear(self.flatten_dim, output_len)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x_flat = x.view(x.size(0), -1)\n",
    "        return self.fc(x_flat)\n",
    "\n",
    "model = LinearModel(input_dim, INPUT_LEN, OUTPUT_LEN)\n",
    "trained_models['Linear'] = train_model(model, 'Linear', train_loader, val_loader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model 2: Multi-Layer Perceptron (MLP)\n",
    "\n",
    "![mlp](imgs/mlp.png)\n",
    "\n",
    "An MLP is a foundational type of neural network that learns relationships between inputs and outputs by passing data through multiple stages of processing.\n",
    "\n",
    "* **Layers:** Think of these as stages in an assembly line. The input layer receives raw data, hidden layers process the data to extract features and patterns, and the output layer delivers the final prediction.\n",
    "* **Nonlinear Activations:** These are simple functions (like ReLU or Sigmoid) applied at each layer. They are the \"spark\" that allows the network to learn complex, curvy patterns. Without them, the network could only understand simple straight-line relationships (linear regression).\n",
    "\n",
    "We add hidden layers with `ReLU` activations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPModel(nn.Module):\n",
    "    def __init__(self, input_dim, input_len, output_len, hidden_dim=64):\n",
    "        super().__init__()\n",
    "        self.flatten_dim = input_dim * input_len\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(self.flatten_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2), # Regularization\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim // 2, output_len)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x_flat = x.view(x.size(0), -1)\n",
    "        return self.net(x_flat)\n",
    "\n",
    "model = MLPModel(input_dim, INPUT_LEN, OUTPUT_LEN)\n",
    "trained_models['MLP'] = train_model(model, 'MLP', train_loader, val_loader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model 3: LSTM (Long Short-Term Memory)\n",
    "\n",
    "![lstm](imgs/lstm.png)\n",
    "\n",
    "LSTMs are a specialized type of Recurrent Neural Network (RNN) designed to learn long-term dependencies in sequence data (like time series, speech, or text). Unlike standard feedforward networks (like MLPs), LSTMs have \"loops\" that allow information to persist.\n",
    "\n",
    "* **The Cell State (Memory):** It allows information to flow unchanged, letting the network remember things from long ago.\n",
    "* **Gates (The Filters):** LSTMs use specific mechanisms called \"gates\" (Forget, Input, and Output) to regulate this flow of information. They act like traffic controllers, deciding what old information to throw away and what new information to store in the long-term memory.\n",
    "\n",
    "The LSTM doesn't flatten time. It walks through the sequence step-by-step, updating its memory.\n",
    "This is theoretically the best approach for physical systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=50, output_len=24, num_layers=2):\n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers=num_layers, batch_first=True, dropout=0.2)\n",
    "        self.fc = nn.Linear(hidden_dim, output_len)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).to(x.device)\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "\n",
    "        last_step_out = out[:, -1, :]\n",
    "        last_step_out = self.dropout(last_step_out)\n",
    "        return self.fc(last_step_out)\n",
    "\n",
    "# Train LSTM\n",
    "model = LSTMModel(input_dim, output_len=OUTPUT_LEN)\n",
    "trained_models['LSTM'] = train_model(model, 'LSTM', train_loader, val_loader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_forecast(idx=0):\n",
    "    # Get a batch from Test Set\n",
    "    x_batch, y_batch = next(iter(test_loader))\n",
    "    \n",
    "    # Get predictions (scaled)\n",
    "    preds_scaled = {}\n",
    "    for name, model in trained_models.items():\n",
    "        model.eval()\n",
    "        try:\n",
    "            device = next(model.parameters()).device\n",
    "        except:\n",
    "            device = 'cpu'\n",
    "            \n",
    "        x_batch_device = x_batch.to(device)\n",
    "        with torch.no_grad():\n",
    "            preds_scaled[name] = model(x_batch_device).cpu().numpy()[idx]\n",
    "    \n",
    "    # Prepare Plot\n",
    "    fig, ax = plt.subplots(figsize=(14, 6))\n",
    "\n",
    "    dataset = test_loader.dataset\n",
    "    history_scaled = dataset.y[idx : idx + INPUT_LEN].numpy()\n",
    "    \n",
    "    # Inverse Transform\n",
    "    history = y_scaler.inverse_transform(history_scaled.reshape(-1, 1)).flatten()\n",
    "    \n",
    "    history_len = len(history)\n",
    "    history_time = range(-history_len, 0)\n",
    "    ax.plot(history_time, history, 'k:', linewidth=2, label='History (Past)')\n",
    "\n",
    "    # 1. Plot Truth (Y)\n",
    "    truth_scaled = y_batch[idx].numpy()\n",
    "    truth = y_scaler.inverse_transform(truth_scaled.reshape(-1, 1)).flatten()\n",
    "    ax.plot(range(0, OUTPUT_LEN), truth, 'k-', linewidth=3, label='Truth (Future)')\n",
    "\n",
    "    # 2. Plot Predictions\n",
    "    colors = {'Linear': 'blue', 'MLP': 'orange', 'LSTM': 'red'}\n",
    "    for name, p_scaled in preds_scaled.items():\n",
    "        # Inverse Transform Prediction\n",
    "        pred = y_scaler.inverse_transform(p_scaled.reshape(-1, 1)).flatten()\n",
    "        \n",
    "        # Calculate RMSE on Original Scale (°C)\n",
    "        rmse = np.sqrt(np.mean((pred - truth)**2))\n",
    "        \n",
    "        ax.plot(range(0, OUTPUT_LEN), pred, color=colors.get(name, 'green'), linestyle='--', \n",
    "                label=f\"{name} (RMSE: {rmse:.2f} °C)\")\n",
    "\n",
    "    ax.set_title(f\"Forecast with History (Test Sample {idx})\")\n",
    "    ax.set_xlabel(\"Hours relative to present\")\n",
    "    ax.set_ylabel(\"Temperature (°C)\")\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.axvline(x=0, color='gray', linestyle='-', alpha=0.5)\n",
    "    plt.show()\n",
    "plot_forecast(idx=20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hidsi_wxclim",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
