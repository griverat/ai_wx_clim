{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Forecasting using weather station data\n",
    "\n",
    "In this notebook, we will build a forecasting model for the weather station data selected.\n",
    "\n",
    "**Goal**: Predict Temperature for the **next 12 hours** using **24 hours** of history."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will explore three model architectures:\n",
    "\n",
    "1.  **Linear Regression (The Baseline)**:\n",
    "    *   *Theory*: \"Tomorrow is just a weighted average of the past.\"\n",
    "    *   *Pros*: Fast, Interpretable.\n",
    "    *   *Cons*: Limited power; cannot learn complex interactions (e.g., \"If wind is high AND sun is out, temp rises fast\").\n",
    "\n",
    "2.  **MLP (Multi-Layer Perceptron)**:\n",
    "    *   *Theory*: \"Non-linear pattern matching.\" Adds hidden layers and activation functions (ReLU) to bend the lines.\n",
    "    *   *Pros*: Can model complex curves.\n",
    "    *   *Cons*: Treats the history as a flat vector. It doesn't inherently understand that Hour 1 came before Hour 2.\n",
    "\n",
    "3.  **LSTM (Long Short-Term Memory)**:\n",
    "    *   *Theory*: \"Sequence Modeling.\" Processes data step-by-step, maintaining an internal \"state\" or memory.\n",
    "    *   *Pros*: Designed for time series; understands causality and long-term dependencies.\n",
    "    *   *Cons*: Slower to train.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Data Preparation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running locally. Skipping dependency installation.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "except:\n",
    "    IN_COLAB = False\n",
    "\n",
    "if IN_COLAB:\n",
    "    print(\"Running on Google Colab. Installing dependencies...\")\n",
    "    !pip install lightning\n",
    "else:\n",
    "    print(\"Running locally. Skipping dependency installation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data\n",
    "We load the processed hourly weather data for SFO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 36219 rows.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>temp</th>\n",
       "      <th>rhum</th>\n",
       "      <th>prcp</th>\n",
       "      <th>snwd</th>\n",
       "      <th>wdir</th>\n",
       "      <th>wspd</th>\n",
       "      <th>wpgt</th>\n",
       "      <th>pres</th>\n",
       "      <th>tsun</th>\n",
       "      <th>cldc</th>\n",
       "      <th>coco</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2022-01-01 00:00:00</th>\n",
       "      <td>11.7</td>\n",
       "      <td>61.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>290.0</td>\n",
       "      <td>27.7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1012.4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-01-01 01:00:00</th>\n",
       "      <td>10.6</td>\n",
       "      <td>68.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>300.0</td>\n",
       "      <td>25.9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1012.8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-01-01 02:00:00</th>\n",
       "      <td>10.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>300.0</td>\n",
       "      <td>18.4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1013.3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-01-01 03:00:00</th>\n",
       "      <td>9.4</td>\n",
       "      <td>71.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>290.0</td>\n",
       "      <td>18.4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1014.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-01-01 04:00:00</th>\n",
       "      <td>9.4</td>\n",
       "      <td>74.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>310.0</td>\n",
       "      <td>18.4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1014.6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     temp  rhum  prcp  snwd   wdir  wspd  wpgt    pres  tsun  \\\n",
       "time                                                                           \n",
       "2022-01-01 00:00:00  11.7  61.0   NaN   NaN  290.0  27.7   NaN  1012.4   NaN   \n",
       "2022-01-01 01:00:00  10.6  68.0   0.0   NaN  300.0  25.9   NaN  1012.8   NaN   \n",
       "2022-01-01 02:00:00  10.0  68.0   0.0   NaN  300.0  18.4   NaN  1013.3   NaN   \n",
       "2022-01-01 03:00:00   9.4  71.0   0.0   NaN  290.0  18.4   NaN  1014.0   NaN   \n",
       "2022-01-01 04:00:00   9.4  74.0   0.0   NaN  310.0  18.4   NaN  1014.6   NaN   \n",
       "\n",
       "                     cldc  coco  \n",
       "time                             \n",
       "2022-01-01 00:00:00   4.0   2.0  \n",
       "2022-01-01 01:00:00   4.0   2.0  \n",
       "2022-01-01 02:00:00   2.0   2.0  \n",
       "2022-01-01 03:00:00   0.0   2.0  \n",
       "2022-01-01 04:00:00   2.0   2.0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = '../data/processed/sfo_hourly_2022_2026.csv'\n",
    "data = pd.read_csv(data_path, index_col='time', parse_dates=True)\n",
    "\n",
    "print(f\"Loaded {len(data)} rows.\")\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before feeding data into models, we must decide what the model should \"see\".\n",
    "\n",
    "*   **Temperature (temp)**: Driven by solar radiation and air mass movements.\n",
    "*   **Humidity (rhum)**: Critical for cloud formation and thermal comfort.\n",
    "*   **Pressure (pres)**: Large-scale high/low pressure systems drive wind and weather patterns.\n",
    "*   **Wind Speed (wspd)**: Advection (moving heat around).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs (4 features): ['pres', 'rhum', 'wspd', 'temp']\n",
      "Target: temp\n"
     ]
    }
   ],
   "source": [
    "INPUT_FEATURES = [\n",
    "    'pres', # Pressure\n",
    "    'rhum', # Humidity\n",
    "    'wspd', # Wind Speed\n",
    "    'temp', # Temperature\n",
    "]\n",
    "\n",
    "TARGET_FEATURE = 'temp' # We want to forecast Temperature\n",
    "\n",
    "# Configuration for Time Sequence\n",
    "HISTORY_HOURS = 24\n",
    "FORECAST_HOURS = 12\n",
    "\n",
    "input_dim = len(INPUT_FEATURES)\n",
    "print(f\"Inputs ({input_dim} features): {INPUT_FEATURES}\")\n",
    "print(f\"Target: {TARGET_FEATURE}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time-Based Splitting & Scaling\n",
    "\n",
    "We cannot shuffle time series data randomly. We fit the Scaler ONLY on the training set to simulate a real-world scenario (we don't know the future range of values)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 21043 rows\n",
      "Val:   5261 rows\n",
      "Test:  9915 rows\n"
     ]
    }
   ],
   "source": [
    "split_date = '2025-01-01'\n",
    "\n",
    "# 1. Split Train+Val (Past) vs Test (Future)\n",
    "train_val_df = data[data.index < split_date]\n",
    "test_df = data[data.index >= split_date]\n",
    "\n",
    "# 2. Further Split Train vs Validation (80/20 chronological)\n",
    "n_train_val = len(train_val_df)\n",
    "train_end = int(n_train_val * 0.8)\n",
    "\n",
    "train_df = train_val_df.iloc[:train_end]\n",
    "val_df = train_val_df.iloc[train_end:]\n",
    "\n",
    "print(f\"Train: {len(train_df)} rows\")\n",
    "print(f\"Val:   {len(val_df)} rows\")\n",
    "print(f\"Test:  {len(test_df)} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Feature Scaling (Independent for X and y)\n",
    "X_scaler = StandardScaler()\n",
    "y_scaler = StandardScaler()\n",
    "\n",
    "# Fit ONLY on Training Data\n",
    "X_train = X_scaler.fit_transform(train_df[INPUT_FEATURES]).astype(np.float32)\n",
    "y_train = y_scaler.fit_transform(train_df[[TARGET_FEATURE]]).astype(np.float32)\n",
    "\n",
    "# Transform Validation and Test using Training statistics\n",
    "X_val = X_scaler.transform(val_df[INPUT_FEATURES]).astype(np.float32)\n",
    "y_val = y_scaler.transform(val_df[[TARGET_FEATURE]]).astype(np.float32)\n",
    "\n",
    "X_test = X_scaler.transform(test_df[INPUT_FEATURES]).astype(np.float32)\n",
    "y_test = y_scaler.transform(test_df[[TARGET_FEATURE]]).astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to chop our continuous timeline into samples:\n",
    "*   **Input**: Window of length `HISTORY_HOURS`.\n",
    "*   **Target**: The *following* window of length `FORECAST_HOURS`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeatherDataset(Dataset):\n",
    "    def __init__(self, X, y, input_len, output_len):\n",
    "        self.X = torch.tensor(X)\n",
    "        self.y = torch.tensor(y)\n",
    "        self.input_len = input_len\n",
    "        self.output_len = output_len\n",
    "\n",
    "    def __len__(self):\n",
    "        # We stop when we can't form a full future window\n",
    "        return len(self.X) - self.input_len - self.output_len\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Input Sequence\n",
    "        x_seq = self.X[idx : idx + self.input_len]\n",
    "        \n",
    "        # Target Sequence (immediately following input)\n",
    "        y_seq = self.y[idx + self.input_len : idx + self.input_len + self.output_len]\n",
    "        \n",
    "        # Remove channel dim for target: (24, 1) -> (24)\n",
    "        return x_seq, y_seq.squeeze(-1)\n",
    "\n",
    "INPUT_LEN = HISTORY_HOURS\n",
    "OUTPUT_LEN = FORECAST_HOURS\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "train_ds = WeatherDataset(X_train, y_train, INPUT_LEN, OUTPUT_LEN)\n",
    "val_ds = WeatherDataset(X_val, y_val, INPUT_LEN, OUTPUT_LEN)\n",
    "test_ds = WeatherDataset(X_test, y_test, INPUT_LEN, OUTPUT_LEN)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True) # Shuffle Train!\n",
    "val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. The Training Loop (PyTorch Lightning)\n",
    "\n",
    "Instead of writing manual training loops, we use `LightningModule`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeatherPredictor(pl.LightningModule):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.criterion = nn.MSELoss()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        loss = self.criterion(self(x), y)\n",
    "        self.log(\"train_loss\", loss, prog_bar=True)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        loss = self.criterion(self(x), y)\n",
    "        self.log(\"val_loss\", loss, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return optim.Adam(self.parameters(), lr=0.001)\n",
    "\n",
    "# Convenience function to train & save best model\n",
    "def train_model(model, name, train_loader, val_loader):\n",
    "    print(f\"\\n=== Training {name} ===\")\n",
    "    predictor = WeatherPredictor(model)\n",
    "    \n",
    "    # Stop if validation loss doesn't improve for 5 epochs\n",
    "    early_stop = EarlyStopping(monitor=\"val_loss\", patience=5)\n",
    "    checkpoint = ModelCheckpoint(monitor=\"val_loss\", dirpath=\"checkpoints\", filename=f\"{name}-best\")\n",
    "    \n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=50,\n",
    "        callbacks=[early_stop, checkpoint], \n",
    "        accelerator='auto',\n",
    "        devices=1, \n",
    "        logger=False,\n",
    "        enable_progress_bar=True\n",
    "    )\n",
    "    trainer.fit(predictor, train_loader, val_loader)\n",
    "    \n",
    "    # Load best weights\n",
    "    best = WeatherPredictor.load_from_checkpoint(checkpoint.best_model_path, model=model)\n",
    "    return best.model\n",
    "\n",
    "trained_models = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model 1: Linear Regression\n",
    "\n",
    "![lr](imgs/linear_regression.png)\n",
    "\n",
    "The simplest approach. We flatten the history into a single vector and learn a weight for every input point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Epoch 20/49 <span style=\"color: #6206e0; text-decoration-color: #6206e0\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”</span> 329/329 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">0:00:01 â€¢ 0:00:00</span> <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; text-decoration: underline\">264.92it/s</span> <span style=\"font-style: italic\">train_loss: 0.187 val_loss: 0.187</span>\n",
       "Validation  <span style=\"color: #6206e0; text-decoration-color: #6206e0\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•¸</span> 81/82   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">0:00:00 â€¢ 0:00:01</span> <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; text-decoration: underline\">416.40it/s</span>                                  \n",
       "</pre>\n"
      ],
      "text/plain": [
       "Epoch 20/49 \u001b[38;2;98;6;224mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m 329/329 \u001b[2m0:00:01 â€¢ 0:00:00\u001b[0m \u001b[2;4m264.92it/s\u001b[0m \u001b[3mtrain_loss: 0.187 val_loss: 0.187\u001b[0m\n",
       "Validation  \u001b[38;2;98;6;224mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[38;2;98;6;224mâ•¸\u001b[0m 81/82   \u001b[2m0:00:00 â€¢ 0:00:01\u001b[0m \u001b[2;4m416.40it/s\u001b[0m                                  \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class LinearModel(nn.Module):\n",
    "    def __init__(self, input_dim, input_len, output_len):\n",
    "        super().__init__()\n",
    "        self.flatten_dim = input_dim * input_len\n",
    "        self.fc = nn.Linear(self.flatten_dim, output_len)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x_flat = x.view(x.size(0), -1)\n",
    "        return self.fc(x_flat)\n",
    "\n",
    "model = LinearModel(input_dim, INPUT_LEN, OUTPUT_LEN)\n",
    "trained_models['Linear'] = train_model(model, 'Linear', train_loader, val_loader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model 2: Multi-Layer Perceptron (MLP)\n",
    "\n",
    "![mlp](imgs/mlp.png)\n",
    "\n",
    "An MLP is a foundational type of neural network that learns relationships between inputs and outputs by passing data through multiple stages of processing.\n",
    "\n",
    "* **Layers:** Think of these as stages in an assembly line. The input layer receives raw data, hidden layers process the data to extract features and patterns, and the output layer delivers the final prediction.\n",
    "* **Nonlinear Activations:** These are simple functions (like ReLU or Sigmoid) applied at each layer. They are the \"spark\" that allows the network to learn complex, curvy patterns. Without them, the network could only understand simple straight-line relationships (linear regression).\n",
    "\n",
    "We add hidden layers with `ReLU` activations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "ğŸ’¡ Tip: For seamless cloud logging and experiment tracking, try installing [litlogger](https://pypi.org/project/litlogger/) to enable LitLogger, which logs metrics and artifacts automatically to the Lightning Experiments platform.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Training MLP ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”“\n",
       "â”ƒ<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">   </span>â”ƒ<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Name      </span>â”ƒ<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Type     </span>â”ƒ<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Params </span>â”ƒ<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Mode  </span>â”ƒ<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> FLOPs </span>â”ƒ\n",
       "â”¡â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”©\n",
       "â”‚<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 0 </span>â”‚ model     â”‚ MLPModel â”‚  8.7 K â”‚ train â”‚     0 â”‚\n",
       "â”‚<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 1 </span>â”‚ criterion â”‚ MSELoss  â”‚      0 â”‚ train â”‚     0 â”‚\n",
       "â””â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”˜\n",
       "</pre>\n"
      ],
      "text/plain": [
       "â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”“\n",
       "â”ƒ\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0mâ”ƒ\u001b[1;35m \u001b[0m\u001b[1;35mName     \u001b[0m\u001b[1;35m \u001b[0mâ”ƒ\u001b[1;35m \u001b[0m\u001b[1;35mType    \u001b[0m\u001b[1;35m \u001b[0mâ”ƒ\u001b[1;35m \u001b[0m\u001b[1;35mParams\u001b[0m\u001b[1;35m \u001b[0mâ”ƒ\u001b[1;35m \u001b[0m\u001b[1;35mMode \u001b[0m\u001b[1;35m \u001b[0mâ”ƒ\u001b[1;35m \u001b[0m\u001b[1;35mFLOPs\u001b[0m\u001b[1;35m \u001b[0mâ”ƒ\n",
       "â”¡â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”©\n",
       "â”‚\u001b[2m \u001b[0m\u001b[2m0\u001b[0m\u001b[2m \u001b[0mâ”‚ model     â”‚ MLPModel â”‚  8.7 K â”‚ train â”‚     0 â”‚\n",
       "â”‚\u001b[2m \u001b[0m\u001b[2m1\u001b[0m\u001b[2m \u001b[0mâ”‚ criterion â”‚ MSELoss  â”‚      0 â”‚ train â”‚     0 â”‚\n",
       "â””â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”˜\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Trainable params</span>: 8.7 K                                                                                            \n",
       "<span style=\"font-weight: bold\">Non-trainable params</span>: 0                                                                                            \n",
       "<span style=\"font-weight: bold\">Total params</span>: 8.7 K                                                                                                \n",
       "<span style=\"font-weight: bold\">Total estimated model params size (MB)</span>: 0                                                                          \n",
       "<span style=\"font-weight: bold\">Modules in train mode</span>: 9                                                                                           \n",
       "<span style=\"font-weight: bold\">Modules in eval mode</span>: 0                                                                                            \n",
       "<span style=\"font-weight: bold\">Total FLOPs</span>: 0                                                                                                     \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mTrainable params\u001b[0m: 8.7 K                                                                                            \n",
       "\u001b[1mNon-trainable params\u001b[0m: 0                                                                                            \n",
       "\u001b[1mTotal params\u001b[0m: 8.7 K                                                                                                \n",
       "\u001b[1mTotal estimated model params size (MB)\u001b[0m: 0                                                                          \n",
       "\u001b[1mModules in train mode\u001b[0m: 9                                                                                           \n",
       "\u001b[1mModules in eval mode\u001b[0m: 0                                                                                            \n",
       "\u001b[1mTotal FLOPs\u001b[0m: 0                                                                                                     \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3ccb6df68264f0aacb494f1b0a56d1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class MLPModel(nn.Module):\n",
    "    def __init__(self, input_dim, input_len, output_len, hidden_dim=64):\n",
    "        super().__init__()\n",
    "        self.flatten_dim = input_dim * input_len\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(self.flatten_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2), # Regularization\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim // 2, output_len)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x_flat = x.view(x.size(0), -1)\n",
    "        return self.net(x_flat)\n",
    "\n",
    "model = MLPModel(input_dim, INPUT_LEN, OUTPUT_LEN)\n",
    "trained_models['MLP'] = train_model(model, 'MLP', train_loader, val_loader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model 3: LSTM (Long Short-Term Memory)\n",
    "\n",
    "![lstm](imgs/lstm.png)\n",
    "\n",
    "LSTMs are a specialized type of Recurrent Neural Network (RNN) designed to learn long-term dependencies in sequence data (like time series, speech, or text). Unlike standard feedforward networks (like MLPs), LSTMs have \"loops\" that allow information to persist.\n",
    "\n",
    "* **The Cell State (Memory):** It allows information to flow unchanged, letting the network remember things from long ago.\n",
    "* **Gates (The Filters):** LSTMs use specific mechanisms called \"gates\" (Forget, Input, and Output) to regulate this flow of information. They act like traffic controllers, deciding what old information to throw away and what new information to store in the long-term memory.\n",
    "\n",
    "The LSTM doesn't flatten time. It walks through the sequence step-by-step, updating its memory.\n",
    "This is theoretically the best approach for physical systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=50, output_len=24, num_layers=2):\n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers=num_layers, batch_first=True, dropout=0.2)\n",
    "        self.fc = nn.Linear(hidden_dim, output_len)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).to(x.device)\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "\n",
    "        last_step_out = out[:, -1, :]\n",
    "        last_step_out = self.dropout(last_step_out)\n",
    "        return self.fc(last_step_out)\n",
    "\n",
    "# Train LSTM\n",
    "model = LSTMModel(input_dim, output_len=OUTPUT_LEN)\n",
    "trained_models['LSTM'] = train_model(model, 'LSTM', train_loader, val_loader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_forecast(idx=0):\n",
    "    # Get a batch from Test Set\n",
    "    x_batch, y_batch = next(iter(test_loader))\n",
    "    \n",
    "    # Get predictions (scaled)\n",
    "    preds_scaled = {}\n",
    "    for name, model in trained_models.items():\n",
    "        model.eval()\n",
    "        try:\n",
    "            device = next(model.parameters()).device\n",
    "        except:\n",
    "            device = 'cpu'\n",
    "            \n",
    "        x_batch_device = x_batch.to(device)\n",
    "        with torch.no_grad():\n",
    "            preds_scaled[name] = model(x_batch_device).cpu().numpy()[idx]\n",
    "    \n",
    "    # Prepare Plot\n",
    "    fig, ax = plt.subplots(figsize=(14, 6))\n",
    "\n",
    "    dataset = test_loader.dataset\n",
    "    history_scaled = dataset.y[idx : idx + INPUT_LEN].numpy()\n",
    "    \n",
    "    # Inverse Transform\n",
    "    history = y_scaler.inverse_transform(history_scaled.reshape(-1, 1)).flatten()\n",
    "    \n",
    "    history_len = len(history)\n",
    "    history_time = range(-history_len, 0)\n",
    "    ax.plot(history_time, history, 'k:', linewidth=2, label='History (Past)')\n",
    "\n",
    "    # 1. Plot Truth (Y)\n",
    "    truth_scaled = y_batch[idx].numpy()\n",
    "    truth = y_scaler.inverse_transform(truth_scaled.reshape(-1, 1)).flatten()\n",
    "    ax.plot(range(0, OUTPUT_LEN), truth, 'k-', linewidth=3, label='Truth (Future)')\n",
    "\n",
    "    # 2. Plot Predictions\n",
    "    colors = {'Linear': 'blue', 'MLP': 'orange', 'LSTM': 'red'}\n",
    "    for name, p_scaled in preds_scaled.items():\n",
    "        # Inverse Transform Prediction\n",
    "        pred = y_scaler.inverse_transform(p_scaled.reshape(-1, 1)).flatten()\n",
    "        \n",
    "        # Calculate RMSE on Original Scale (Â°C)\n",
    "        rmse = np.sqrt(np.mean((pred - truth)**2))\n",
    "        \n",
    "        ax.plot(range(0, OUTPUT_LEN), pred, color=colors.get(name, 'green'), linestyle='--', \n",
    "                label=f\"{name} (RMSE: {rmse:.2f} Â°C)\")\n",
    "\n",
    "    ax.set_title(f\"Forecast with History (Test Sample {idx})\")\n",
    "    ax.set_xlabel(\"Hours relative to present\")\n",
    "    ax.set_ylabel(\"Temperature (Â°C)\")\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.axvline(x=0, color='gray', linestyle='-', alpha=0.5)\n",
    "    plt.show()\n",
    "plot_forecast(idx=20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (hidsi_wxclim)",
   "language": "python",
   "name": "hidsi_wxclim"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
