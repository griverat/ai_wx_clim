{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Data Exploration: Hawaii Rainfall Data\n",
                "\n",
                "**Goal**: Explore the available HCDP rainfall data, understand its structure, and prepare a dataset for the workshop.\n",
                "\n",
                "We have data from 1990 to 2026 arranged in folders by Year -> Month."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import cartopy.crs as ccrs\n",
                "import glob\n",
                "import os\n",
                "from tqdm import tqdm\n",
                "\n",
                "plt.rcParams[\"font.family\"] = \"monospace\""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Define path to data\n",
                "DATA_ROOT = \"../data/HCDP_data/rainfall/new/day/statewide/partial/station_data\"\n",
                "\n",
                "# Check if path exists\n",
                "if os.path.exists(DATA_ROOT):\n",
                "    print(f\"Data root found: {DATA_ROOT}\")\n",
                "else:\n",
                "    print(f\"WARNING: Data root not found at {DATA_ROOT}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Load Data\n",
                "\n",
                "We will iterate through the directories and load a subset of data to understand the structure. Loading 30+ years might take time, so let's start by listing files."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "all_files = glob.glob(os.path.join(DATA_ROOT, \"*\", \"*\", \"*.csv\"))\n",
                "all_files = sorted(all_files)\n",
                "\n",
                "print(f\"Total CSV files found: {len(all_files)}\")\n",
                "print(\"Sample files:\", all_files[:3])"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Load one file to inspect columns"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "sample_df = pd.read_csv(all_files[0])\n",
                "print(\"Shape:\", sample_df.shape)\n",
                "sample_df.head()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "We can plot the `(lat,lon)` pairs to have a general overview of the location of the stations"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "fig, ax = plt.subplots(subplot_kw=dict(projection=ccrs.PlateCarree()))\n",
                "sample_df.plot.scatter(\"LON\", \"LAT\", ax=ax, s=2, c=\"r\")\n",
                "gl = ax.gridlines(draw_labels=True, ls=\"--\", lw=0.5)\n",
                "ax.coastlines()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "The data is in **Wide Format**. Each day of the month is a column.\n",
                "\n",
                "## 2. Extract Data for a Target Station\n",
                "\n",
                "We want to predict rainfall for a specific location. Let's find a station with good data coverage. \n",
                "**Honolulu International Airport** is usually a reliable station. Let's look for it."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Search for HNL Airport in the sample\n",
                "hnl_station = sample_df[sample_df['Station.Name'].str.contains(\"HONOLULU INTERNATIONAL\", case=False, na=False)]\n",
                "hnl_station"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "If found, we will trace this specific `SKN` (Station Key Number) across all files to build a time series."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "TARGET_SKN = 703 # Honolulu International Airport\n",
                "\n",
                "def extract_station_timeseries(skn, file_list):\n",
                "    daily_rainfall = {}\n",
                "    \n",
                "    for file_path in tqdm(file_list, desc=\"Processing files\"):\n",
                "        try:\n",
                "            df = pd.read_csv(file_path)\n",
                "            # Filter\n",
                "            station_data = df[df['SKN'] == skn]\n",
                "            \n",
                "            if not station_data.empty:\n",
                "                # Extract date columns (start with 'X')\n",
                "                date_cols = [c for c in df.columns if c.startswith('X')]\n",
                "                \n",
                "                for col in date_cols:\n",
                "                    # Format: X1990.01.01\n",
                "                    date_str = col[1:] # Remove 'X'\n",
                "                    val = station_data.iloc[0][col]\n",
                "                    daily_rainfall[date_str] = val\n",
                "        except Exception as e:\n",
                "            print(f\"Error reading {file_path}: {e}\")\n",
                "            continue\n",
                "            \n",
                "    return daily_rainfall\n",
                "\n",
                "# For the workshop preparation, we want to see full extent availability.\n",
                "rainfall_data = extract_station_timeseries(TARGET_SKN, all_files)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Visualize & Clean\n",
                "\n",
                "Convert to DataFrame and fix dates."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "ts_df = pd.DataFrame.from_dict(rainfall_data, orient='index', columns=['Rainfall_mm'])\n",
                "ts_df.index = pd.to_datetime(ts_df.index, format='%Y.%m.%d')\n",
                "ts_df = ts_df.sort_index()\n",
                "\n",
                "print(f\"Extracted {len(ts_df)} days of data.\")\n",
                "print(\"Missing values:\", ts_df['Rainfall_mm'].isna().sum())"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Handle Missing Values (Interpolate)\n",
                "ts_df['Rainfall_mm_filled'] = ts_df['Rainfall_mm'].interpolate(method='time')\n",
                "print(\"Missing values:\", ts_df['Rainfall_mm_filled'].isna().sum())"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Now lets visualize the data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "fig, ax = plt.subplots(figsize=(15, 5))\n",
                "ax.bar(ts_df.index, ts_df['Rainfall_mm_filled'], 10, label='Rainfall (Interpolated)', alpha=0.7)\n",
                "ax.set_title(f\"Daily Rainfall for Station SKN {TARGET_SKN} (1990-202x)\")\n",
                "ax.set_ylabel(\"Rainfall (mm)\")\n",
                "ax.legend()\n",
                "ax.set_xlim(ts_df.index.min(), ts_df.index.max())"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Export data\n",
                "\n",
                "We can save this processed time-series as a single CSV `station_703_rainfall.csv` so we have an already consolidated dataset to work with."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "ts_df.to_csv(\"../data/processed/station_703_daily_rainfall.csv\")\n",
                "print(\"Saved consolidated dataset.\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "pangeo",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.9"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
